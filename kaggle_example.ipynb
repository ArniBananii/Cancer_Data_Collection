{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T16:29:11.275956Z",
     "start_time": "2025-05-23T16:28:45.052751Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import Augmentor\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "data_dir = \"./Data\"  # Ordner mit Unterordnern: Train/, Test/\n",
    "train_dir = os.path.join(data_dir, \"Train\")\n",
    "test_dir = os.path.join(data_dir, \"Test\")\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "num_classes = 9\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== AUGMENTOR PIPELINE ==========\n",
    "\n",
    "class_names = [d for d in os.listdir(train_dir) if not d.startswith('.')]\n",
    "path_to_training_dataset = \"Data/Train\"\n",
    "\n",
    "for i in class_names:\n",
    "    source_dir = os.path.join(path_to_training_dataset, i)\n",
    "    p = Augmentor.Pipeline(source_dir, output_directory=\"output\")\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.flip_left_right(probability=0.5)\n",
    "    p.zoom_random(probability=0.5, percentage_area=0.8)\n",
    "    p.random_contrast(probability=0.5, min_factor=0.7, max_factor=1.3)\n",
    "    p.sample(500)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 438 image(s) found.\n",
      "Output directory set to Data/Train/melanoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x3105D54D0>: 100%|██████████| 500/500 [00:06<00:00, 78.62 Samples/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 462 image(s) found.\n",
      "Output directory set to Data/Train/pigmented benign keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x3104C19D0>: 100%|██████████| 500/500 [00:01<00:00, 315.55 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 357 image(s) found.\n",
      "Output directory set to Data/Train/nevus/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=2530x2122 at 0x310344190>: 100%|██████████| 500/500 [00:05<00:00, 84.75 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 376 image(s) found.\n",
      "Output directory set to Data/Train/basal cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x310478FD0>: 100%|██████████| 500/500 [00:02<00:00, 228.71 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 114 image(s) found.\n",
      "Output directory set to Data/Train/actinic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x3252E6C50>: 100%|██████████| 500/500 [00:01<00:00, 322.56 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 181 image(s) found.\n",
      "Output directory set to Data/Train/squamous cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x1723E9290>: 100%|██████████| 500/500 [00:01<00:00, 303.17 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 139 image(s) found.\n",
      "Output directory set to Data/Train/vascular lesion/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x310473750>: 100%|██████████| 500/500 [00:01<00:00, 305.28 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 77 image(s) found.\n",
      "Output directory set to Data/Train/seborrheic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x310024650>: 100%|██████████| 500/500 [00:03<00:00, 157.55 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 95 image(s) found.\n",
      "Output directory set to Data/Train/dermatofibroma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x31003F010>: 100%|██████████| 500/500 [00:01<00:00, 292.14 Samples/s]                  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:43:43.504319Z",
     "start_time": "2025-05-23T16:43:42.816152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== TRANSFORMS ==========\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ========== DATASETS & LOADERS ==========\n",
    "image_datasets = {\n",
    "    \"train\": ImageFolder(train_dir, transform=data_transforms[\"train\"]),\n",
    "    \"val\": ImageFolder(test_dir, transform=data_transforms[\"val\"])\n",
    "}\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    for x in [\"train\", \"val\"]\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "# ========== MODEL SETUP ==========\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ========== TRAINING LOOP ==========\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history\n",
    "\n",
    "# ========== START TRAINING ==========\n",
    "model, train_loss, val_loss, train_acc, val_acc = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# ========== PLOTTING ==========\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(val_loss, label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label=\"Train Acc\")\n",
    "plt.plot(val_acc, label=\"Val Acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "706b69269ee24be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 2611, in <module>\n",
      "    from torch import _meta_registrations\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_meta_registrations.py\", line 12, in <module>\n",
      "    from torch._decomp import (\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_decomp/__init__.py\", line 276, in <module>\n",
      "    import torch._decomp.decompositions\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_decomp/decompositions.py\", line 16, in <module>\n",
      "    import torch._prims as prims\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_prims/__init__.py\", line 525, in <module>\n",
      "    abs = _make_elementwise_unary_prim(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_prims/__init__.py\", line 493, in _make_elementwise_unary_prim\n",
      "    return _make_prim(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_prims/__init__.py\", line 321, in _make_prim\n",
      "    prim_def = torch.library.custom_op(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_library/custom_ops.py\", line 173, in custom_op\n",
      "    return inner(fn)\n",
      "           ^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_library/custom_ops.py\", line 154, in inner\n",
      "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_library/custom_ops.py\", line 204, in __init__\n",
      "    self._register_to_dispatcher()\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_library/custom_ops.py\", line 624, in _register_to_dispatcher\n",
      "    lib._register_fake(self._name, fake_impl, _stacklevel=4)\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/library.py\", line 193, in _register_fake\n",
      "    source = torch._library.utils.get_source(_stacklevel + 1)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jakobmerten/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/_library/utils.py\", line 54, in get_source\n",
      "    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py\", line 1071, in findsource\n",
      "    module = getmodule(object, file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py\", line 997, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen posixpath>\", line 416, in realpath\n",
      "  File \"<frozen posixpath>\", line 451, in _joinrealpath\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 91\u001B[39m\n\u001B[32m     88\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m model, train_loss_history, val_loss_history, train_acc_history, val_acc_history\n\u001B[32m     90\u001B[39m \u001B[38;5;66;03m# ========== START TRAINING ==========\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m model, train_loss, val_loss, train_acc, val_acc = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[38;5;66;03m# ========== PLOTTING ==========\u001B[39;00m\n\u001B[32m     94\u001B[39m plt.figure(figsize=(\u001B[32m12\u001B[39m, \u001B[32m5\u001B[39m))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 52\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, criterion, optimizer, num_epochs)\u001B[39m\n\u001B[32m     49\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m     50\u001B[39m running_corrects = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[43mphase\u001B[49m\u001B[43m]\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:493\u001B[39m, in \u001B[36mDataLoader.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    491\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._iterator\n\u001B[32m    492\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m493\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:424\u001B[39m, in \u001B[36mDataLoader._get_iterator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    422\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    423\u001B[39m     \u001B[38;5;28mself\u001B[39m.check_worker_number_rationality()\n\u001B[32m--> \u001B[39m\u001B[32m424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/uni/ai/Cancer_Data_Collection/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1171\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter.__init__\u001B[39m\u001B[34m(self, loader)\u001B[39m\n\u001B[32m   1164\u001B[39m w.daemon = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   1165\u001B[39m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[32m   1166\u001B[39m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[32m   1167\u001B[39m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[32m   1168\u001B[39m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[32m   1169\u001B[39m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[32m   1170\u001B[39m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1171\u001B[39m \u001B[43mw\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1172\u001B[39m \u001B[38;5;28mself\u001B[39m._index_queues.append(index_queue)\n\u001B[32m   1173\u001B[39m \u001B[38;5;28mself\u001B[39m._workers.append(w)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001B[39m, in \u001B[36mBaseProcess.start\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process._config.get(\u001B[33m'\u001B[39m\u001B[33mdaemon\u001B[39m\u001B[33m'\u001B[39m), \\\n\u001B[32m    119\u001B[39m        \u001B[33m'\u001B[39m\u001B[33mdaemonic processes are not allowed to have children\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    120\u001B[39m _cleanup()\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m \u001B[38;5;28mself\u001B[39m._popen = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[38;5;28mself\u001B[39m._sentinel = \u001B[38;5;28mself\u001B[39m._popen.sentinel\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001B[39m, in \u001B[36mProcess._Popen\u001B[39m\u001B[34m(process_obj)\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_Popen\u001B[39m(process_obj):\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mProcess\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001B[39m, in \u001B[36mSpawnProcess._Popen\u001B[39m\u001B[34m(process_obj)\u001B[39m\n\u001B[32m    285\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    286\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_Popen\u001B[39m(process_obj):\n\u001B[32m    287\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpopen_spawn_posix\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001B[39m, in \u001B[36mPopen.__init__\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[32m     31\u001B[39m     \u001B[38;5;28mself\u001B[39m._fds = []\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001B[39m, in \u001B[36mPopen.__init__\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mself\u001B[39m.returncode = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;28mself\u001B[39m.finalizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001B[39m, in \u001B[36mPopen._launch\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     60\u001B[39m     \u001B[38;5;28mself\u001B[39m.sentinel = parent_r\n\u001B[32m     61\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[33m'\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m'\u001B[39m, closefd=\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m         f.write(fp.getbuffer())\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     64\u001B[39m     fds_to_close = []\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== TEST-SET AUSWERTUNG ==========\n",
    "def evaluate_on_test(model, test_dir, transform, batch_size=32):\n",
    "    model.eval()  # Evaluation Mode\n",
    "\n",
    "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\n✅ Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# ========== TESTEN ==========\n",
    "evaluate_on_test(model, test_dir=test_dir, transform=data_transforms[\"val\"])"
   ],
   "id": "9a666791b002ee76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
