{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T16:29:11.275956Z",
     "start_time": "2025-05-23T16:28:45.052751Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import Augmentor\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "data_dir = \"./Data\"  # Ordner mit Unterordnern: Train/, Test/\n",
    "train_dir = os.path.join(data_dir, \"Train\")\n",
    "test_dir = os.path.join(data_dir, \"Test\")\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "num_classes = 9\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== AUGMENTOR PIPELINE ==========\n",
    "\n",
    "class_names = [d for d in os.listdir(train_dir) if not d.startswith('.')]\n",
    "path_to_training_dataset = \"Data/Train\"\n",
    "\n",
    "for i in class_names:\n",
    "    source_dir = os.path.join(path_to_training_dataset, i)\n",
    "    p = Augmentor.Pipeline(source_dir, output_directory=\"output\")\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.flip_left_right(probability=0.5)\n",
    "    p.zoom_random(probability=0.5, percentage_area=0.8)\n",
    "    p.random_contrast(probability=0.5, min_factor=0.7, max_factor=1.3)\n",
    "    p.sample(500)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 438 image(s) found.\n",
      "Output directory set to Data/Train/melanoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x3105D54D0>: 100%|██████████| 500/500 [00:06<00:00, 78.62 Samples/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 462 image(s) found.\n",
      "Output directory set to Data/Train/pigmented benign keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x3104C19D0>: 100%|██████████| 500/500 [00:01<00:00, 315.55 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 357 image(s) found.\n",
      "Output directory set to Data/Train/nevus/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=2530x2122 at 0x310344190>: 100%|██████████| 500/500 [00:05<00:00, 84.75 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 376 image(s) found.\n",
      "Output directory set to Data/Train/basal cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x310478FD0>: 100%|██████████| 500/500 [00:02<00:00, 228.71 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 114 image(s) found.\n",
      "Output directory set to Data/Train/actinic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x3252E6C50>: 100%|██████████| 500/500 [00:01<00:00, 322.56 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 181 image(s) found.\n",
      "Output directory set to Data/Train/squamous cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x1723E9290>: 100%|██████████| 500/500 [00:01<00:00, 303.17 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 139 image(s) found.\n",
      "Output directory set to Data/Train/vascular lesion/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x310473750>: 100%|██████████| 500/500 [00:01<00:00, 305.28 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 77 image(s) found.\n",
      "Output directory set to Data/Train/seborrheic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x310024650>: 100%|██████████| 500/500 [00:03<00:00, 157.55 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 95 image(s) found.\n",
      "Output directory set to Data/Train/dermatofibroma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x31003F010>: 100%|██████████| 500/500 [00:01<00:00, 292.14 Samples/s]                  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-23T16:29:22.946450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== TRANSFORMS ==========\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ========== DATASETS & LOADERS ==========\n",
    "image_datasets = {\n",
    "    \"train\": ImageFolder(train_dir, transform=data_transforms[\"train\"]),\n",
    "    \"val\": ImageFolder(test_dir, transform=data_transforms[\"val\"])\n",
    "}\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    for x in [\"train\", \"val\"]\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "# ========== MODEL SETUP ==========\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ========== TRAINING LOOP ==========\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history\n",
    "\n",
    "# ========== START TRAINING ==========\n",
    "model, train_loss, val_loss, train_acc, val_acc = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# ========== PLOTTING ==========\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(val_loss, label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label=\"Train Acc\")\n",
    "plt.plot(val_acc, label=\"Val Acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "706b69269ee24be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== TEST-SET AUSWERTUNG ==========\n",
    "def evaluate_on_test(model, test_dir, transform, batch_size=32):\n",
    "    model.eval()  # Evaluation Mode\n",
    "\n",
    "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\n✅ Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# ========== TESTEN ==========\n",
    "evaluate_on_test(model, test_dir=test_dir, transform=data_transforms[\"val\"])"
   ],
   "id": "9a666791b002ee76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
